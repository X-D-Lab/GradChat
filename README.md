<h1 align="center">GradChat(é”¦é²¤): è€ƒè¯•å¤§æ¨¡å‹</h1>  

## æ¨ç†è„šæœ¬  

```
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

model_dir = snapshot_download("thomas/test", revision = 'v1.0.10')

tokenizer = AutoTokenizer.from_pretrained(model_dir,trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", bf16=True, trust_remote_code=True).eval()

model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True) 

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)

```

## ğŸ‘¨â€ğŸ’» ç ”å‘å›¢é˜Ÿ

æœ¬é¡¹ç›®ç”±**åä¸œç†å·¥å¤§å­¦ X-D Lab**è¯¾é¢˜ç»„å‘èµ·:
| ä¸»è¦åˆ†å·¥ | å‚ä¸åŒå­¦ |
| :----: | :---- |
| æ¨¡å‹è®­ç»ƒ | [é¢œé‘«](https://github.com/thomas-yanxin)ã€[å”äº•æ¥ ](https://github.com/jingnant) |
| æ¨¡å‹æµ‹è¯• |  |
| æ•°æ®æ„å»º |  |
